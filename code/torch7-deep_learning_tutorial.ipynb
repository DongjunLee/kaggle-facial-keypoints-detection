{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using convolutional neural nets to detect facial keypoints tutorial\n",
    "### implement this in torch7\n",
    "---\n",
    "\n",
    "\n",
    "## see detail tutorial : [kaggle-tutorial](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "I assume you have the just torch7, csvigo, image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Setting data's config.\n",
    "\n",
    "IMG_DIM = 96\n",
    "\n",
    "MAX_PIXEL_VAL = 255.0\n",
    "MAX_FEATURE = 30\n",
    "MAX_TRAIN_IMG = 7049\n",
    "MAX_TEST_IMG = 1783\n",
    "MAX_TEST_OUTPUT = 27124\n",
    "\n",
    "FILEPATH_DATA_DIR = \"../data/\"\n",
    "FILEPATH_TRAIN = FILEPATH_DATA_DIR..\"training.csv\"\n",
    "FILEPATH_TEST = FILEPATH_DATA_DIR..\"test.csv\"\n",
    "FILEPATH_TEST_FEATURE = FILEPATH_DATA_DIR..\"IdLookupTable.csv\"\n",
    "FILEPATH_TEST_OUTPUT = FILEPATH_DATA_DIR..\"test_output.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Lua Library about csv\n",
    "require 'csvigo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvFile = io.open(FILEPATH_TRAIN, 'r')\n",
    "header = csvFile:read()\n",
    "feature_map = header:split(',')\n",
    "\n",
    "-- original index 31 -> \"Image\"  but, there is error.\n",
    "-- So remove and insert.\n",
    "table.remove(feature_map, 31)\n",
    "table.insert(feature_map, \"Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Define \n",
    "feature_data = torch.Tensor(MAX_TRAIN_IMG, MAX_FEATURE)\n",
    "image_data = torch.Tensor(MAX_TRAIN_IMG, IMG_DIM*IMG_DIM)\n",
    "csvigoFile = csvigo.load(FILEPATH_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i=1, MAX_TRAIN_IMG do\n",
    "    image_index = MAX_FEATURE+1\n",
    "    local x = csvigoFile[feature_map[image_index]][i]\n",
    "    local image = x:split(' ')\n",
    "    \n",
    "    -- Scale pixel values to [0, 1]\n",
    "    image_data[i] = torch.Tensor(image)/MAX_PIXEL_VAL\n",
    "\n",
    "    image_feature = {}\n",
    "\n",
    "    for j=1, MAX_FEATURE do\n",
    "        local point_info = csvigoFile[feature_map[j]]\n",
    "        local x = tonumber(point_info[i])\n",
    "        if(x ~= nil) then\n",
    "            image_feature[j] = x/IMG_DIM\n",
    "        else\n",
    "            -- values are missing\n",
    "            image_feature[j] = -1\n",
    "        end\n",
    "    end\n",
    "    feature_data[i] = torch.Tensor(image_feature)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(feature_data[3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Prepare data\n",
    "torch.save(FILEPATH_DATA_DIR .. \"feature_data.raw\", feature_data, 'binary')\n",
    "torch.save(FILEPATH_DATA_DIR .. \"image_data.raw\", image_data, 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Load data\n",
    "feature_data = torch.load(FILEPATH_DATA_DIR..\"feature_data.raw\", 'binary')\n",
    "image_data = torch.load(FILEPATH_DATA_DIR..\"image_data.raw\", 'binary')\n",
    "\n",
    "csvFile = io.open(FILEPATH_TRAIN, 'r')\n",
    "header = csvFile:read()\n",
    "csvFile:close()\n",
    "feature_map = header:split(',')\n",
    "\n",
    "table.remove(feature_map, 31)\n",
    "table.insert(feature_map, \"Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The training dataset for the Facial Keypoint Detection challenge consists of 7,049 96x96 gray-scale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "showImage = function(index)\n",
    "    \n",
    "    -- 1D => 2D Tensor\n",
    "    temp = torch.Tensor(IMG_DIM, IMG_DIM)\n",
    "    \n",
    "    imagedata = image_data[index]\n",
    "    for j=1, IMG_DIM do\n",
    "        table_x = {}\n",
    "        for k=1, IMG_DIM do\n",
    "            table.insert(table_x, imagedata[k + (j-1)*IMG_DIM])\n",
    "        end\n",
    "        temp[j] = torch.Tensor(table_x)\n",
    "    end\n",
    "    \n",
    "    -- Represent Feature\n",
    "    BLACK = 0; WHITE = 1\n",
    "    featuredata = feature_data[index]\n",
    "    \n",
    "    for i=1, 30, 2 do\n",
    "        point_x = torch.round(featuredata[i] * IMG_DIM * 100) / 100\n",
    "        point_y = torch.round(featuredata[i+1] * IMG_DIM * 100) / 100\n",
    "        \n",
    "        --print(feature_map[i], featuredata[i], featuredata[i] * IMG_DIM, point_x)\n",
    "        --print(feature_map[i+1], featuredata[i+1], featuredata[i+1] * IMG_DIM, point_y)\n",
    "        \n",
    "        temp[point_y][point_x] = BLACK\n",
    "        temp[point_y-1][point_x] = WHITE\n",
    "        temp[point_y+1][point_x] = WHITE\n",
    "        temp[point_y][point_x-1] = WHITE\n",
    "        temp[point_y][point_x+1] = WHITE\n",
    "    end\n",
    "    \n",
    "    itorch.image(temp)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "showImage(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not necessary that you go through every single detail of this function. But let's take a look at what the script above outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i=1, MAX_FEATURE do\n",
    "    local byte_vec = torch.ne(feature_data:select(2, i), -1.0)\n",
    "    print (feature_map[i], torch.sum(byte_vec))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's only 2,140 images in the dataset that have all 30 target values present. Initially, we'll train with these 2,140 samples only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_images = 0\n",
    "image_id_map = {}\n",
    "\n",
    "for i=1, MAX_TRAIN_IMG do\n",
    "    local byte_vec = torch.ne(feature_data:select(1, i), -1.0)\n",
    "    if torch.sum(byte_vec) == MAX_FEATURE then\n",
    "        num_images = num_images + 1\n",
    "        image_id_map[num_images] = i\n",
    "    end\n",
    "end\n",
    "print(\"Num images with all the 30 feature vectors : \" .. num_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "image reference : http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/\n",
    "\n",
    "![original](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)\n",
    "![flipped](http://danielnouri.org/media/kfkd/samples3.png)\n",
    "\n",
    "Image =>    Original / Flipped \n",
    "\n",
    "Since we're flipping the images, we'll have to make sure we also flip the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flip_indices = { \n",
    "    {1, 3}, {2, 4},\n",
    "    {5, 9}, {6, 10}, {7, 11}, {8, 12},\n",
    "    {13, 17}, {14, 18}, {15, 19}, {16, 20},\n",
    "    {23, 25}, {24, 26} \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i=1, #flip_indices do\n",
    "    original = flip_indices[i][1]\n",
    "    flipped = flip_indices[i][2]\n",
    "    print(\"#\", feature_map[original], \"->\" , feature_map[flipped])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flip_feature_data = torch.Tensor(MAX_TRAIN_IMG, MAX_FEATURE)\n",
    "flip_image_data = torch.Tensor(MAX_TRAIN_IMG, IMG_DIM*IMG_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i=1, MAX_TRAIN_IMG do\n",
    "    local x = image_data[i]\n",
    "    flip_x = {}\n",
    "    \n",
    "    -- Flip left to right\n",
    "    for j=1, IMG_DIM do\n",
    "        for k = IMG_DIM, 1, -1 do\n",
    "            table.insert(flip_x, x[k + (j-1)*IMG_DIM])\n",
    "        end\n",
    "    end\n",
    "    flip_image_data[i] = torch.Tensor(flip_x)\n",
    "    \n",
    "    local y = feature_data[i]\n",
    "    flip_y = {}\n",
    "    for j=1, MAX_FEATURE do\n",
    "        index = j\n",
    "        for k=1, #flip_indices do\n",
    "            original = flip_indices[k][1]\n",
    "            flipped = flip_indices[k][2]\n",
    "\n",
    "            if index == original then index = flipped break end\n",
    "            if index == flipped then index = original break end\n",
    "        end\n",
    "        \n",
    "        if index%2 == 1 then\n",
    "            if y[index] ~= -1 then\n",
    "                table.insert(flip_y, 1 - y[index])\n",
    "            else\n",
    "                table.insert(flip_y, -1)\n",
    "            end\n",
    "        else\n",
    "            table.insert(flip_y, y[index])\n",
    "        end\n",
    "    end\n",
    "    flip_feature_data[i] = torch.Tensor(flip_y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Prepare flipped data\n",
    "torch.save(FILEPATH_DATA_DIR .. \"flip_feature_data.raw\", flip_feature_data, 'binary')\n",
    "torch.save(FILEPATH_DATA_DIR .. \"flip_image_data.raw\", flip_image_data, 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Load flipped data\n",
    "flip_feature_data = torch.load(FILEPATH_DATA_DIR..\"flip_feature_data.raw\", 'binary')\n",
    "flip_image_data = torch.load(FILEPATH_DATA_DIR..\"flip_image_data.raw\", 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Load Image.\n",
    "loadImageWithFeature = function(index, imagedata, featuredata)\n",
    "    temp = torch.Tensor(IMG_DIM, IMG_DIM)\n",
    "    \n",
    "    image_pixels = imagedata[index]\n",
    "\n",
    "    for j=1, IMG_DIM do\n",
    "        table_x = {}\n",
    "        for k=1, IMG_DIM do\n",
    "            table.insert(table_x, image_pixels[k + (j-1)*IMG_DIM])\n",
    "        end\n",
    "        temp[j] = torch.Tensor(table_x)\n",
    "    end\n",
    "    \n",
    "    -- Represent Feature\n",
    "    BLACK = 0; WHITE = 1\n",
    "    feature_points = featuredata[index]\n",
    "    \n",
    "    for i=1, 30, 2 do\n",
    "        point_x = torch.round(feature_points[i] * IMG_DIM * 100) / 100\n",
    "        point_y = torch.round(feature_points[i+1] * IMG_DIM * 100) / 100\n",
    "        \n",
    "        temp[point_y][point_x] = BLACK\n",
    "        temp[point_y-1][point_x] = WHITE\n",
    "        temp[point_y+1][point_x] = WHITE\n",
    "        temp[point_y][point_x-1] = WHITE\n",
    "        temp[point_y][point_x+1] = WHITE\n",
    "    end\n",
    "    \n",
    "    return temp\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Check flip image.\n",
    "image1 = loadImageWithFeature(1, image_data, feature_data)\n",
    "flip_image1 = loadImageWithFeature(1, flip_image_data, flip_feature_data)\n",
    "itorch.image({image1, flip_image1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Reshape 1D -> 2D\n",
    "for i=1, MAX_TRAIN_IMG do\n",
    "    image_data[i] = torch.reshape(image_data[i], 96, 96)\n",
    "    flip_image_data[i] = torch.reshape(flip_image_data[i], 96, 96)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'image'\n",
    "require 'optim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Convolutional Network\n",
    "\n",
    "model = nn.Sequential()\n",
    "\n",
    "nfeats = 1\n",
    "nstates = {32, 64, 128, 500, 500}\n",
    "filtsize = 3\n",
    "padding = (filtsize-1)/2\n",
    "poolsize = 2\n",
    "noutputs = MAX_FEATURE\n",
    "\n",
    "-- stage 1 : filter bank -> squashing -> L2 pooling\n",
    "model:add(nn.SpatialConvolutionMM(nfeats, nstates[1], filtsize, filtsize, 1, 1, padding, padding))\n",
    "model:add(nn.ReLU())\n",
    "model:add(nn.SpatialMaxPooling(poolsize,poolsize,poolsize,poolsize))\n",
    "\n",
    "-- stage 2 : filter bank -> squashing -> L2 pooling\n",
    "model:add(nn.SpatialConvolutionMM(nstates[1], nstates[2], filtsize, filtsize, 1, 1, padding, padding))\n",
    "model:add(nn.ReLU())\n",
    "model:add(nn.SpatialMaxPooling(poolsize,poolsize,poolsize,poolsize))\n",
    "\n",
    "-- stage 2 : filter bank -> squashing -> L2 pooling\n",
    "model:add(nn.SpatialConvolutionMM(nstates[2], nstates[3], filtsize, filtsize, 1, 1, padding, padding))\n",
    "model:add(nn.ReLU())\n",
    "model:add(nn.SpatialMaxPooling(poolsize,poolsize,poolsize,poolsize))\n",
    "\n",
    "-- stage 4 : standard 3-layer neural network\n",
    "model:add(nn.View(nstates[3]*12*12))\n",
    "model:add(nn.Dropout(0.5))\n",
    "model:add(nn.Linear(nstates[3]*12*12, nstates[4]))\n",
    "model:add(nn.ReLU())\n",
    "model:add(nn.Linear(nstates[4], nstates[5]))\n",
    "model:add(nn.ReLU())\n",
    "model:add(nn.Linear(nstates[5], noutputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_images_training = math.floor((80*num_images)/100)\n",
    "num_images_validating = num_images - num_images_training\n",
    "print(\"Num Train Images : \" .. num_images_training .. \" Num Validating Images : \" .. num_images_validating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batchSize = 64  -- 1 : pure stochastic\n",
    "epochSize = 1000\n",
    "threadNum = 3\n",
    "seedNum = 1\n",
    "\n",
    "torch.setnumthreads(threadNum)\n",
    "torch.manualSeed(seedNum)\n",
    "\n",
    "criterion = nn.MSECriterion()\n",
    "x, dl_dx = model:getParameters()\n",
    "\n",
    "sgd_params = {\n",
    "   learningRate = .01, --1e-3,\n",
    "   learningRateDecay = .001, --1e-4,\n",
    "   weightDecay = 0,\n",
    "   momentum = .9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feval = function(x_new)\n",
    "    if x ~= x_new then\n",
    "        x:copy(x_new)\n",
    "    end\n",
    "    \n",
    "    dl_dx:zero()\n",
    "    local loss_x = 0\n",
    "    \n",
    "    for batch_num = 1, batchSize do\n",
    "        _nidx_ = (_nidx or 0) + 1\n",
    "        if _nidx_ > num_images_training then _nidx_ = 1 end\n",
    "        \n",
    "        if _nidx_ % 50 == 0 then\n",
    "            collectgarbage()\n",
    "        end\n",
    "        \n",
    "        local image_id = image_id_map[shuffle_idx[_nidx_]]\n",
    "        \n",
    "        local inputs1 = image_data[image_id]:view(1, 96, 96)\n",
    "        local target1 = feature_data[image_id]\n",
    "        \n",
    "        local loss1 = criterion:forward(model:forward(inputs1), target1)\n",
    "        model:backward(inputs1, criterion:backward(model.output, target1))\n",
    "        loss_x = loss_x + loss1\n",
    "        \n",
    "        local inputs2 = flip_image_data[image_id]:view(1, 96, 96)\n",
    "        local target2 = flip_feature_data[image_id]\n",
    "        \n",
    "        local loss2 = criterion:forward(model:forward(inputs2), target2)\n",
    "        model:backward(inputs2, criterion:backward(model.output, target2))\n",
    "        loss_x = loss_x + loss2\n",
    "    end\n",
    "    \n",
    "    loss_x = loss_x/batchSize\n",
    "    dl_dx = dl_dx:div(batchSize)\n",
    "    \n",
    "    return loss_x, dl_dx\n",
    "end    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch =1, epochSize do\n",
    "    model:training()\n",
    "    shuffle_idx = torch.randperm(num_images_training)\n",
    "    \n",
    "    current_loss = 0\n",
    "    \n",
    "    local time = sys.clock()\n",
    "    for img_id = 1, num_images_training, batchSize do\n",
    "        _, fs = optim.nag(feval, x, sgd_params)\n",
    "        current_loss = current_loss + math.sqrt(fs[1])\n",
    "    end\n",
    "    \n",
    "    time = sys.clock() - time\n",
    "    time = time / num_images_training\n",
    "    \n",
    "    current_loss = current_loss * batchSize / num_images_training\n",
    "    print(epoch .. ' current loss = ' .. current_loss)\n",
    "    \n",
    "    model:evaluate()\n",
    "    local validation_loss = 0.0\n",
    "    start_time = sys.clock()\n",
    "    for i = num_images_training, num_images do\n",
    "        local image_id = image_id_map[i]\n",
    "        inputs = image_data[image_id]:view(1, 96, 96)\n",
    "        \n",
    "        local target = feature_data[image_id]\n",
    "        local forward_output = model:forward(inputs)\n",
    "        \n",
    "        local byte_vec_fea = torch.ne(feature_data:select(1, image_id), -1.0)\n",
    "        local byte_vec_non_fea = torch.eq(feature_data:select(1, image_id), -1.0)\n",
    "        local zeroed_target = torch.cmul(target:double(), byte_vec_fea:double())\n",
    "        local selected_output = torch.cmul(forward_output:double(), byte_vec_non_fea:double())\n",
    "        local equalised_target = torch.add(zeroed_target:double(), selected_output:double())\n",
    "        \n",
    "        local error = equalised_target - forward_output\n",
    "        local mse = torch.norm(error) / math.sqrt(torch.sum(byte_vec_fea))\n",
    "        validation_loss = validation_loss + mse\n",
    "    end\n",
    "    print(epoch.. ' current validation loss ' .. validation_loss / num_images_validating)\n",
    "    \n",
    "    time = sys.clock() - start_time\n",
    "    time = time / num_images_training\n",
    "    print(\"==> time to validate 1 sample = \" .. (time * 1000) .. ' ms')\n",
    "    \n",
    "    if(epoch % 50 == 0) then\n",
    "        modsav = model:clone('weight', 'bias')\n",
    "        torch.save(FILEPATH_DATA_DIR .. 'trained_model_' .. epoch .. '.t7' , modsav)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.setdefaulttensortype('torch.FloatTensor')\n",
    "\n",
    "inv_feature_map = {}\n",
    "for i=1, MAX_FEATURE do\n",
    "    inv_feature_map[feature_map[i]] = i\n",
    "end\n",
    "\n",
    "testImageFile = csvigo.load(FILEPATH_TEST)\n",
    "testFeatureFile = csvigo.load(FILEPATH_TEST_FEATURE)\n",
    "\n",
    "test_data = torch.Tensor(MAX_TEST_IMG, IMG_DIM*IMG_DIM)\n",
    "feature_data = torch.Tensor(MAX_TEST_IMG, MAX_FEATURE)\n",
    "savedModel = torch.load(FILEPATH_DATA_DIR .. \"trained_model_400.t7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local validation_loss = 0.0\n",
    "for i=1, MAX_TEST_IMG do\n",
    "    local x = testImageFile[\"Image\"][i]\n",
    "    local image = x:split(' ')\n",
    "    local input_1d = torch.Tensor(image)/MAX_PIXEL_VAL; test_data[i] = input_1d\n",
    "    local inputs = input_1d:view(1, 96, 96):type('torch.FloatTensor')\n",
    "    inputs = inputs:double()\n",
    "    local myPrediction = savedModel:forward(inputs)\n",
    "    feature_data[i] = torch.Tensor(myPrediction:float())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testOutputFile = csvigo.File(FILEPATH_TEST_OUTPUT, \"w\")\n",
    "testOutputFile:write({\"RowId\", \"Location\"})\n",
    "\n",
    "function trim1(s)\n",
    "    return (s:gsub(\"^%s*(.-)%s*$\", \"%1\"))\n",
    "end\n",
    "\n",
    "for i=1, MAX_TEST_OUTPUT do\n",
    "    local imageId = testFeatureFile[\"ImageId\"][i]\n",
    "    local featureId = inv_feature_map[trim1(testFeatureFile[\"FeatureName\"][i])]\n",
    "    local location = feature_data[imageId][featureId]*96\n",
    "    if(location > 95) then location = 95; end\n",
    "    if(location < 0) then location = 0; end\n",
    "    \n",
    "    print(imageId, trim1(testFeatureFile[\"FeatureName\"][i]), location)\n",
    "    \n",
    "    testOutputFile:write({i, location})\n",
    "end\n",
    "testOutputFile:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_image = loadImageWithFeature(1000, test_data, feature_data)\n",
    "itorch.image(predict_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
